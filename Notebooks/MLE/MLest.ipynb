{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Likelihood Estimation (MACS 30100 and MACS 40200)\n",
    "### by [Richard W. Evans](https://sites.google.com/site/rickecon/), January 2018\n",
    "The code in this Jupyter notebook was written using Python 3.6. It also uses data file `Econ381totpts.txt` and the image file `GBtree.png`. For the code to run properly, you should have the data file and the image file in the same folder as the Jupyter notebook file. Otherwise, you will have to change the respective lines of the code that read in the data and the image to reflect the location of that data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. General characterization of a model and data generating process\n",
    "Each of the model estimation approaches that we will discuss in this section on Maximum Likelihood estimation (MLE) and in subsequent sections on generalized method of moments (GMM) and simulated method of moments (SMM) involves choosing values of the parameters of a model to make the model match some number of properties of the data. Define a model or a data generating process (DGP) as:\n",
    "\n",
    "$$ F(X|\\theta) = 0 \\quad\\text{or}\\quad F(x_1, x_2,...x_J|\\theta) = 0 $$\n",
    "\n",
    "In reality, a model could also include inequalities representing constraints. But this is sufficient for our discussion. The goal of maximum likelihood estimation (MLE) is to choose the parameter vector of the model $\\theta$ to maximize the likelihood of seeing the data produced by the model $(x_1, x_2,... x_J)$.\n",
    "\n",
    "An example of an economic model that follows the more general definition of $F(X|\\theta) = 0$ is Brock and Mirman (1972). This model has multiple nonlinear dynamic equations, 7 parameters, 1 exogenous time series of variables, and about 5 endogenous time series of variables. A maximum likelihood model with\n",
    "\n",
    "Another example of a model is a statistical distribution [e.g., the normal distribution $N(\\mu, \\sigma)$].\n",
    "\n",
    "$$ Pr(x|\\theta) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}} $$\n",
    "\n",
    "The probability of drawing value $x_i$ from the distribution $f(x|\\theta)$ is $f(x_i|\\theta)$. The probability of drawing the following vector of two observations $(x_1,x_2)$ from the distribution $f(x|\\theta)$ is $f(x_1|\\theta)\\times f(x_2|\\theta)$. We define the likelihood function of $N$ draws $(x_1,x_2,...x_N)$ from a model or distribution $f(x|\\theta)$ as $\\mathcal{L}$.\n",
    "\n",
    "$$ \\mathcal{L}(x_1,x_2,...x_N|\\theta) \\equiv \\prod_{i=1}^N f(x_i|\\theta) $$\n",
    "\n",
    "Because it can be numerically difficult to maximize a product of percentages (one small value can make dominate the entire product), it is almost always easier to use the log likelihood function $\\ln(\\mathcal{L})$.\n",
    "\n",
    "$$ \\ln\\Bigl(\\mathcal{L}(x_1,x_2,...x_N|\\theta)\\Bigr) \\equiv \\sum_{i=1}^N \\ln\\Bigl(f(x_i|\\theta)\\Bigr) $$\n",
    "\n",
    "The maximum likelihood estimate $\\hat{\\theta}_{MLE}$ is the following:\n",
    "\n",
    "$$ \\hat{\\theta}_{MLE} = \\theta:\\quad \\max_\\theta \\: \\ln\\mathcal{L} = \\sum_{i=1}^N\\ln\\Bigl(f(x_i|\\theta)\\Bigr) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Comparisons of distributions and data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import some data from the total points earned by all the students in two sections of my intermediate macroeconomics class for undergraduates at my previous University in a certain year (two semesters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as sts\n",
    "\n",
    "pts = np.loadtxt('Econ381totpts.txt')\n",
    "pts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a histogram of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# This next command is specifically for Jupyter Notebook\n",
    "%matplotlib notebook\n",
    "count, bins, ignored = plt.hist(pts, 30, edgecolor='black', normed=True)\n",
    "plt.title('Econ 381 scores: 2011-2012', fontsize=20)\n",
    "plt.xlabel('Total points')\n",
    "plt.ylabel('Percent of scores')\n",
    "plt.xlim([0, 550])  # This gives the xmin and xmax to be plotted\"\n",
    "# Don't set this figure until have played with the figure below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a distribution around these data that we think fits it well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define function that generates values of a potentially trucated normal\n",
    "# probability density function (PDF)\n",
    "def truncnorm_pdf(xvals, mu, sigma, cutoff):\n",
    "    '''\n",
    "    --------------------------------------------------------------------\n",
    "    Generate pdf values from the truncated normal pdf with mean mu and\n",
    "    standard deviation sigma. If the cutoff is finite, then the PDF\n",
    "    values are inflated upward to reflect the zero probability on values\n",
    "    above the cutoff. If there is no cutoff given or if it is given as\n",
    "    infinity, this function does the same thing as\n",
    "    sp.stats.norm.pdf(x, loc=mu, scale=sigma).\n",
    "    --------------------------------------------------------------------\n",
    "    INPUTS:\n",
    "    xvals  = (N,) vector, values of the normally distributed random\n",
    "             variable\n",
    "    mu     = scalar, mean of the normally distributed random variable\n",
    "    sigma  = scalar > 0, standard deviation of the normally distributed\n",
    "             random variable\n",
    "    cutoff = scalar or string, ='None' if no cutoff is given, otherwise\n",
    "             is scalar upper bound value of distribution. Values above\n",
    "             this value have zero probability\n",
    "    \n",
    "    OTHER FUNCTIONS AND FILES CALLED BY THIS FUNCTION: None\n",
    "    \n",
    "    OBJECTS CREATED WITHIN FUNCTION:\n",
    "    prob_notcut = scalar \n",
    "    pdf_vals = (N,) vector, normal PDF values for mu and sigma\n",
    "               corresponding to xvals data\n",
    "    \n",
    "    FILES CREATED BY THIS FUNCTION: None\n",
    "    \n",
    "    RETURNS: pdf_vals\n",
    "    --------------------------------------------------------------------\n",
    "    '''\n",
    "    if cutoff == 'None':\n",
    "        prob_notcut = 1.0 - sts.norm.cdf(0, loc=mu, scale=sigma)\n",
    "    else:\n",
    "        prob_notcut = (sts.norm.cdf(cutoff, loc=mu, scale=sigma) -\n",
    "                       sts.norm.cdf(0, loc=mu, scale=sigma))\n",
    "            \n",
    "    pdf_vals    = ((1/(sigma * np.sqrt(2 * np.pi)) *\n",
    "                    np.exp( - (xvals - mu)**2 / (2 * sigma**2))) /\n",
    "                    prob_notcut)\n",
    "    \n",
    "    return pdf_vals\n",
    "\n",
    "dist_pts = np.linspace(0, 450, 500)\n",
    "mu_1 = 300\n",
    "sig_1 = 100\n",
    "plt.plot(dist_pts, truncnorm_pdf(dist_pts, mu_1, sig_1, 450),\n",
    "         linewidth=2, color='r', label='1: $\\mu$=300,$\\sigma$=100')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "mu_2 = 400\n",
    "sig_2 = 30\n",
    "plt.plot(dist_pts, truncnorm_pdf(dist_pts, mu_2, sig_2, 450),\n",
    "         linewidth=2, color='g', label='2: $\\mu$=400,$\\sigma$=30')\n",
    "plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which distribution will have the biggest log likelihood function? Why?\n",
    "\n",
    "Let's compute the log likelihood function for this data for both of these distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define log likelihood function for the normal distribution\n",
    "def log_lik_truncnorm(xvals, mu, sigma, cutoff):\n",
    "    pdf_vals = truncnorm_pdf(xvals, mu, sigma, cutoff)\n",
    "    ln_pdf_vals = np.log(pdf_vals)\n",
    "    log_lik_val = ln_pdf_vals.sum()\n",
    "    \n",
    "    return log_lik_val\n",
    "\n",
    "print('Log-likelihood 1: ', log_lik_truncnorm(pts, mu_1, sig_1, 450))\n",
    "print('Log-likelihood 2: ', log_lik_truncnorm(pts, mu_2, sig_2, 450))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is the log likelihood value negative?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we estimate $\\mu$ and $\\sigma$ by maximum likelihood? What values of $\\mu$ and $\\sigma$ will maximize the likelihood function?\n",
    "$$(\\hat{\\mu},\\hat{\\sigma})_{MLE} = (\\mu, \\sigma):\\quad argmax_{\\mu,\\sigma}\\:\\mathcal{L}=\\sum_{i=1}^N\\ln\\Bigl(f(x_i|\\mu,\\sigma)\\Bigr)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. How to set up a maximization (minimization) problem in Python\n",
    "A minimizer is a function that chooses a single value or a vector of values to minimize the result of a scalar-valued function of that vector. Any maximization problem can be restated as a minimization problem. Because minimization problems are more numerically stable and well defined, most numerical optimizers are stated as minimizers. The [scipy.optimize](https://docs.scipy.org/doc/scipy-0.18.1/reference/optimize.html) library has many types of root-finders and minimizers. For our maximum likelihood estimation problems, we will use the [scipy.optimize.minimize()](https://docs.scipy.org/doc/scipy-0.18.1/reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize) function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. The criterion function\n",
    "The first step is to write a function that takes two inputs and returns a scalar value.\n",
    "1. The first input is either a scalar or a vector of values (the object `params` in the function `crit()` below). This object is the value or values being chosen to minimize the criterion function.\n",
    "2. The second object is Python's variable length input objects `*args`, which is a tuple of variable length positional arguments. As you will see in the `minimize()` function, all the arguments must be passed into the criterion function in one tuple.\n",
    "3. Lastly, you must make sure that the scalar criterion value that the function returns is the value of the problem stated as a minimization problem and not a maximization problem. In this case of maximum likelihood estimation, you want the negative of the log likelihood function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crit(params, *args):\n",
    "    mu, sigma = params\n",
    "    xvals, cutoff = args\n",
    "    log_lik_val = log_lik_truncnorm(xvals, mu, sigma, cutoff)\n",
    "    neg_log_lik_val = -log_lik_val\n",
    "    \n",
    "    return neg_log_lik_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. The minimize() function\n",
    "The `minimize()` function is shorthand for `scipy.optimize.minimize()`. This function returns a dictionary of objects including the solution to the optimization problem and whether the problem actually solved. The `minimize` function has three mandatory arguments, plus a lot of options. You can experiment with the options on the [`minimize()` documentation page](https://docs.scipy.org/doc/scipy-0.18.1/reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize).\n",
    "1. The first argument of the minimize function is the criterion function (`crit()` in this example) from which the `minimize()` function will test values of the parameters in searching for the minimum value.\n",
    "2. The second argument is an initial guess for the values of the parameters that minimize the criterion function `crit()`.\n",
    "3. The third argument is the tuple of all the objects needed to solve the criterion function in `crit()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.optimize as opt\n",
    "\n",
    "mu_init = 400  # mu_2\n",
    "sig_init = 70  # sig_2\n",
    "params_init = np.array([mu_init, sig_init])\n",
    "mle_args = (pts, 450.0)\n",
    "results = opt.minimize(crit, params_init, args=(mle_args))\n",
    "mu_MLE, sig_MLE = results.x\n",
    "print('mu_MLE=', mu_MLE, ' sig_MLE=', sig_MLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot the histogram of the data\n",
    "count, bins, ignored = plt.hist(pts, 30, edgecolor='black', normed=True)\n",
    "plt.title('Econ 381 scores: 2011-2012', fontsize=20)\n",
    "plt.xlabel('Total points')\n",
    "plt.ylabel('Percent of scores')\n",
    "plt.xlim([0, 550])  # This gives the xmin and xmax to be plotted\"\n",
    "\n",
    "# Plot the two test distributions from before\n",
    "plt.plot(dist_pts, truncnorm_pdf(dist_pts, mu_1, sig_1, 450),\n",
    "         linewidth=2, color='r', label='1: $\\mu$=300,$\\sigma$=30')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "plt.plot(dist_pts, truncnorm_pdf(dist_pts, mu_2, sig_2, 450),\n",
    "         linewidth=2, color='g', label='2: $\\mu$=400,$\\sigma$=70')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "# Plot the MLE estimated distribution\n",
    "plt.plot(dist_pts, truncnorm_pdf(dist_pts, mu_MLE, sig_MLE, 450),\n",
    "         linewidth=2, color='k', label='3: $\\mu$=380,$\\sigma$=55')\n",
    "plt.legend(loc='upper left')\n",
    "plt.savefig('MLEplots.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Log-likelihood 1: ', log_lik_truncnorm(pts, mu_1, sig_1, 450))\n",
    "print('Log-likelihood 2: ', log_lik_truncnorm(pts, mu_2, sig_2, 450))\n",
    "print('MLE log-likelihood 3: ', log_lik_truncnorm(pts, mu_MLE, sig_MLE, 450))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if this likelihood function is well behaved by looking at a grid over possible values of $\\mu$ and $\\sigma$ for the given data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "cmap1 = matplotlib.cm.get_cmap('summer')\n",
    "\n",
    "mu_vals = np.linspace(615, 627, 50)\n",
    "sig_vals = np.linspace(193, 204, 50)\n",
    "lnlik_vals = np.zeros((50, 50))\n",
    "for mu_ind in range(50):\n",
    "    for sig_ind in range(50):\n",
    "        lnlik_vals[mu_ind, sig_ind] = log_lik_truncnorm(pts, mu_vals[mu_ind],\n",
    "                                                        sig_vals[sig_ind], 450)\n",
    "\n",
    "mu_mesh, sig_mesh = np.meshgrid(mu_vals, sig_vals)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "ax.plot_surface(sig_mesh, mu_mesh, lnlik_vals, rstride=8,\n",
    "                cstride=1, cmap=cmap1)\n",
    "ax.set_title('Log likelihood for values of mu and sigma')\n",
    "ax.set_xlabel(r'$\\sigma$')\n",
    "ax.set_ylabel(r'$\\mu$')\n",
    "ax.set_zlabel(r'log likelihood')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Constrained minimization using the minimize() function\n",
    "The [`minimize()`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html) function has many methods that can be used to find the parameter values that minimize some criterion function. These methods are called using the `method='MethodName'` optional input argument to the minimize function. Three of those methods allow for constrained minimization by providing upper and lower bounds for the parameters being chosen. These three methods are `'L-BFGS-B'`, `'TNC'`, and `'SLSQP'`.\n",
    "\n",
    "Suppose you were trying to estimate $\\mu$ and $\\sigma$ of a truncated normal distribution as is the case above. The value of $\\mu$ need not be constrained. However, the value of $\\sigma$ must be strictly positive. You could include these bounds in a constrained minimization by using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = opt.minimize(crit, params_init, args=(mle_args), method='L-BFGS-B',\n",
    "                       bounds=((1e-10, 420.0), (1e-10, None)))\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you must set the lower bound of $\\sigma$ equal to some small positive number close to zero. You cannot set it to zero itself because the bounds are inclusive. That is, the minimizer might try a value of $\\sigma=0$ is the lower bound includes zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The variance-covariance matrix of ML estimates\n",
    "Davidson and MacKinnon (2004, Sec. 10.4) have a great discussion four different estimators for the variance-covariance matrix of the maximum likelihood estimates. That is, we want to know what is the variance or uncertainty of our estimates for $\\mu$ and $\\sigma$, and how are those two estimates correlated. The four most common estimators for the VCV matrix of a maximum likelihood estimate are:\n",
    "1. Empirical Hessian estimator (H)\n",
    "2. Information matrix estimator (I)\n",
    "3. Outer-product-of-the-gradient estimator (OPG)\n",
    "4. Sandwich estimator (S)\n",
    "\n",
    "All of these estimators of the VCV matrix intuitively measure how flat the likelihood function is at the estimated parameter values in the dimension of each estimated parameter. The Hessian is a matrix of second derivatives of the log likelihood function with respect to the parameters being chosen. The Hessian matrix therefore captures information about how the slope of the log likelihood function is changing in each direction. The empirical Hessian estimator is the most commonly used. One really nice property of Python's `minimize()` function is that one of the result objects is the inverse Hessian.\n",
    "\n",
    "$$ \\hat{VCV}_H(\\hat{\\theta}) =-H^{-1}(\\hat{\\theta}) \\quad\\text{from max prob} $$\n",
    "\n",
    "Note that this expression is for the maximum likelihood problem posed as a maximization problem of the log likelihood function. We are transforming it into a minimization problem of the negative of the log likelihood function. Let the operator $OffDiag(-1)$ change the sign of each off diagonal element. Then the estimator of the variance covariance matrix from the minimization problem is:\n",
    "\n",
    "$$ \\hat{VCV}_H(\\hat(\\theta)) = OffDiag(-1) H^{-1}(\\hat{\\theta}) \\quad\\text{from min prob} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results\n",
    "OffDiagNeg = np.array([[1, -1], [-1, 1]])\n",
    "vcv_mle = results.hess_inv * OffDiagNeg\n",
    "stderr_mu_mle = np.sqrt(vcv_mle[0,0])\n",
    "stderr_sig_mle = np.sqrt(vcv_mle[1,1])\n",
    "print('VCV(MLE) = ', vcv_mle)\n",
    "print('Standard error for mu estimate = ', stderr_mu_mle)\n",
    "print('Standard error for sigma estimate = ', stderr_sig_mle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hypothesis testing\n",
    "Can we reject the hypothesis that $\\mu=400$ and $\\sigma=70$ with 95% confidence? How do you answer that question? What does the figure tell us about this answer? In this section, we will discuss four ways to perform hypothesis testing.\n",
    "1. Two standard errors (back of the envelope, approximation)\n",
    "2. Likelihood ratio test\n",
    "3. Wald test\n",
    "4. Lagrange multiplier test\n",
    "\n",
    "Davidson and MacKinnon (2004, Sec. 10.5) have a more detailed discussion of methods 2, 3, and 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Back of the envelope, two standard errors (assuming normality)\n",
    "A really quick approach to hypothesis testing is to see if your hypothesized values are within two standard errors of the estimated values. This approach is not completely correct because estimates in the log likelihood function are not symmetrically distributed. But it is at least a first approximation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lb_mu_95pctci = mu_MLE - 2 * stderr_mu_mle\n",
    "print('mu_MLE=', mu_MLE, ', lower bound 95% conf. int.=', lb_mu_95pctci)\n",
    "\n",
    "lb_sig_95pctci = sig_MLE - 2 * stderr_sig_mle\n",
    "print('sig_MLE=', sig_MLE, ', lower bound 95% conf. int.=', lb_sig_95pctci)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Likelihood ratio test\n",
    "The likelihood ratio test is the simplest and, therefore, the most common of the three more precise methods (2, 3, and 4). Let your maximum likelihood estimation have $p$ parameters (the vector $\\theta$ has $p$ elements), let $\\hat{\\theta}_{MLE}$ be the maximum likelihood estimate, and let $\\tilde{\\theta}$ be your hypothesized values of the parameters. The likelihood ratio test statistic is the following.\n",
    "\n",
    "$$ LR(\\tilde{\\theta}|\\hat{\\theta}_{MLE}) = 2\\Bigl(\\ln\\ell(\\hat{\\theta}_{MLE}) - \\ln\\ell(\\tilde{\\theta})\\Bigr) \\sim \\chi^2(p) $$\n",
    "\n",
    "Note that this is a joint test of the likelihood of $H_0: \\mu_0, \\sigma_0$. The value of the $\\chi^2(p)$ has the following interpretation. The area under the $\\chi^2(p)$ pdf from $LR$ and above is the significance level or $p$-value. It represents the probability that the null hypothesis $\\tilde{\\theta}$ is true given the MLE estimate $\\hat{\\theta}_{MLE}$. More precisely, it represents the probability of null hypotheses with LR test statistics greater than or equal to (worse) the LR statistic from the null hypothese $\\tilde{\\theta}$. When this $p$-value is small, it it highly unlikely that the null hypothesis is true. You can calculate the $\\chi^2(p)$ significance level by taking one minus the cdf of $\\chi^2(p)$ at the $LR$ value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_lik_h0 = log_lik_truncnorm(pts, 500, 180, 450)\n",
    "log_lik_mle = log_lik_truncnorm(pts, mu_MLE, sig_MLE, 450)\n",
    "LR_val = 2 * (log_lik_mle - log_lik_h0)\n",
    "pval_h0 = 1.0 - sts.chi2.cdf(LR_val, 2)\n",
    "print('chi squared of H0 with 2 degrees of freedom p-value = ', pval_h0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Linear regression with MLE\n",
    "Although linear regression is most often performed using the ordinary least squares (OLS) estimator, which is a particular type of generalized method of moments (GMM) estimator, this can also be done using MLE. A simple regression specification in which the dependent variable $y_i$ is a linear function of two independent variables $x_{1,i}$ and $x_{2,i}$ is the following:\n",
    "\n",
    "$$ y_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + \\varepsilon_i \\quad\\text{where}\\quad \\varepsilon_i\\sim N\\left(0,\\sigma^2\\right) $$\n",
    "\n",
    "If we solve this regression equation for the error term $\\varepsilon_i$, we can start to see how we might estimate the parameters of the model by maximum likelihood.\n",
    "\n",
    "$$ \\varepsilon_i = y_i - \\beta_0 - \\beta_1 x_{1,i} - \\beta_2 x_{2,i} \\sim N\\left(0,\\sigma^2\\right) $$\n",
    "\n",
    "The parameters of the regression model are $(\\beta_0, \\beta_1, \\beta_2, \\sigma)$. Given some data $(y_i, x_{1,i}, x_{2,i})$ and given some parameter values $(\\beta_0, \\beta_1, \\beta_2, \\sigma)$, we could plot a histogram of the distribution of those error terms. And we could compare that empirical histogram to the assumed histogram of the distribution of the errors $N(0,\\sigma^2)$. ML estimation of this regression equation is to choose the paramters $(\\beta_0, \\beta_1, \\beta_2, \\sigma)$ to make that empirical distribution of errors $\\varepsilon_i$ most closely match the assumed distribution of errors $N(0,\\sigma^2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generalized beta family of distributions\n",
    "For exercises in this section, you will need to know the functional forms of four continuous univariate probability density functions (PDF's), each of which are part of the generalized beta family of distributions. The figure below is the generalized beta family of distributions [taken from McDonald and Xu (1995, Fig. 2)].\n",
    "\n",
    "![title](GBtree.png)\n",
    "\n",
    "The lognormal distribution (LN) is the distribution of the exponential of a normally distributed variable with mean $\\mu$ and standard deviation $\\sigma$. If the variable $x_i$ is lognormally distributed $x_i\\sim LN(\\mu,\\sigma)$, then the log of $x_i$ is normally distributed $\\ln(x_i)\\sim N(\\mu,\\sigma)$. The PDF of the lognormal distribution is the following.\n",
    "\n",
    "$$ \\text{(LN):}\\quad f(x;\\mu,\\sigma) = \\frac{1}{x\\sigma\\sqrt{2\\pi}}e^{-\\frac{[\\ln(x)-\\mu]^2}{2\\sigma^2}},\\quad x\\in(0,\\infty), \\:\\mu\\in(-\\infty,\\infty),\\: \\sigma>0 $$\n",
    "\n",
    "Note that the lognormal distribution has a support that is strictly positive. This is one reason why it is commonly used to approximate income distributions. A household's total income is rarely negative. The lognormal distribution also has a lot of the nice properties of the normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another two-parameter distribution with strictly positive support is the gamma (GA) distribution. The pdf of the gamma distribution is the following.\n",
    "\n",
    "$$ \\text{(GA):}\\quad f(x;\\alpha,\\beta) = \\frac{1}{\\beta^\\alpha \\Gamma(\\alpha)}x^{\\alpha-1}e^{-\\frac{x}{\\beta}},\\quad x\\in[0,\\infty), \\:\\alpha,\\beta>0 $$\n",
    "$$ \\text{where}\\quad \\Gamma(z)\\equiv\\int_0^\\infty t^{z-1}e^{-t}dt $$\n",
    "\n",
    "The gamma function $\\Gamma(\\cdot)$ within the gamma (GA) distribution is a common function that has a preprogrammed function in most programming languages.\n",
    "\n",
    "The lognormal (LN) and gamma (GA) distributions are both two-parameter distributions and are both special cases of the three-parameter generalized gamma (GG) distribution. The pdf of the generalized gamma distribution is the following.\n",
    "\n",
    "$$ \\text{(GG):}\\quad f(x;\\alpha,\\beta,m) = \\frac{m}{\\beta^\\alpha \\Gamma\\left(\\frac{\\alpha}{m}\\right)}x^{\\alpha-1}e^{-\\left(\\frac{x}{\\beta}\\right)^m},\\quad x\\in[0,\\infty), \\:\\alpha,\\beta,m>0 $$\n",
    "$$ \\text{where}\\quad \\Gamma(z)\\equiv\\int_0^\\infty t^{z-1}e^{-t}dt $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relationship between the generalized gamma (GG) distribution and the gamma (GA) distribution is straightforward. The GA distribution equals the GG distribution at $m=1$.\n",
    "\n",
    "$$ GA(\\alpha,\\beta) = GG(\\alpha,\\beta,m=1) $$\n",
    "\n",
    "The relationship between the generalized gamma (GG) distribution and the lognormal (LN) distribution is less straightforward. The LN distribution equals the GG distribution as $\\alpha$ goes to zero, $\\beta = (\\alpha\\sigma)^{\\frac{2}{\\alpha}}$, and $m = \\frac{\\alpha\\mu+1}{\\alpha^2\\sigma^2}$. See McDonald, et al (2013) for derivation.\n",
    "\n",
    "$$ LN(\\mu,\\sigma) = \\lim_{\\alpha\\rightarrow 0}GG\\left(\\alpha,\\beta=(\\alpha\\sigma)^{\\frac{2}{\\alpha}},m=\\frac{\\alpha\\mu+1}{\\alpha^2\\sigma^2}\\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last distribution we describe is the generalized beta 2 (GB2) distribution. Like the GG, GA, and LN distributions, it also has a strictly positive support. The PDF of the generalized beta 2 distribution is the following.\n",
    "\n",
    "$$ \\text{(GB2):}\\quad f(x;a,b,p,q) = \\frac{a x^{ap-1}}{b^{ap}B(p,q)\\left(1 + \\left(\\frac{x}{b}\\right)^a\\right)^{p+q}},\\quad x\\in[0,\\infty), \\:a,b,p,q>0 $$\n",
    "$$ \\quad\\text{where}\\quad B(v,w)\\equiv\\int_0^1 t^{v-1}(1-t)^{w-1}dt $$\n",
    "\n",
    "The beta function $B(\\cdot,\\cdot)$ within the GB2 distribution is a common function that has a preprogrammed function in most programming languages. The three-parameter generalized gamma (GG) distribution is a nested case of the four-parameter generalized beta 2 (GB2) distribution as $q$ goes to $\\infty$ and for $a=m$, $b=q^{1/m}\\beta$, and $p=\\frac{\\alpha}{m}$. See McDonald (1984, p. 662) for a derivation.\n",
    "\n",
    "$$ GG(\\alpha,\\beta,m) = \\lim_{q\\rightarrow\\infty}GB2\\left(a=m,b=q^{1/m}\\beta,p=\\frac{\\alpha}{m},q\\right) $$\n",
    "\n",
    "The statistical family tree figure above shows the all the relationships between the various PDF's in the generalized beta family of distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. References\n",
    "* Brock, William A. and Leonard J. Mirman, \"Optimal Economic Growth and Uncertainty: The Discounted Case,\" *Journal of Economic Theory*, 4:3, pp. 479-513 (June 1972).\n",
    "* Davidson, Russell and James G. MacKinnon, *Econometric Theory and Methods*, Oxford University Press (2004).\n",
    "* McDonald, James B., \"Some Generalized Functions for the Size Distribution of Income,\" *Econometrica* 52:3, pp. 647-665 (May 1984).\n",
    "* McDonald, James B. and Yexiao Xu, \"A Generalization of the Beta Distribution with Applications,\" *Journal of Econometrics*, 66:1-2, pp. 133-152 (March-April 1995).\n",
    "* McDonald, James B., Jeff Sorensen, and Patrick A. Turley, \"Skewness and Kurtosis Properties of Income Distribution Models,\" *Review of Income and Wealth*, 59:2, pp. 360-374 (June 2013)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
